{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Number/Size of Decision Trees\n",
    "Gradient boosting involves the creation and addition of decision trees sequentially, each at- tempting to correct the mistakes of the learners that came before it. This raises the question as to how many trees (weak learners or estimators) to configure in your gradient boosting model and how big each tree should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Number of Trees\n",
    "\n",
    "Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. The general reason is that on most problems, adding more trees beyond a limit does not improve the performance of the model. The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees. Quickly, the model reaches a point of diminishing returns.\n",
    "\n",
    "The number of trees (or rounds) in an XGBoost model is specified to the XGBClassifier or XGBRegressor class in the n estimators argument. The default in the XGBoost library is 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning N-estimators with Otto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Library\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data and Split X/Y; encode target class\n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:, 94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "model = XGBClassifier()\n",
    "n_estimators = range(50, 400, 50)\n",
    "param_grid = dict(n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state = 7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold)\n",
    "grid_results = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: r%\" % (mean, stdev, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(50, 400, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot\n",
    "pt.errorbar(n_estimators, means, yerr=stds)\n",
    "pt.title(\"XGBoost n_estimats vs. Log Loss\")\n",
    "pt.xlabel('n_estimators')\n",
    "pt.ylabel('Log Loss')\n",
    "pt.savefig('n_estimators.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Size of Trees\n",
    "\n",
    "In gradient boosting, we can control the size of decision trees, also called the number of layers or the depth. Shallow trees are expected to have poor performance because they capture few details of the problem and are generally referred to as weak learners. Deeper trees generally capture too many details of the problem and overfit the training dataset, limiting the ability to make good predictions on new data. Generally, boosting algorithms are configured with weak learners, decision trees with few layers, sometimes as simple as just a root node, also called a decision stump rather than a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as pt\n",
    "\n",
    "#Load Data and split X/Y; encode Target Class\n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:, 94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifier()\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state=7)\n",
    "max_depth = range(1,11,2)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_log_loss', n_jobs=1, cv=kfold, verbose=1)\n",
    "grid_results = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" %(grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "pt.errorbar(max_depth, means, yerr=stds)\n",
    "pt.title(\"XGBoost max_depth vs. Log Loss\")\n",
    "pyplot.xlabel('max_depth')\n",
    "pyplot.xlabel('Log Loss')\n",
    "pyplot.savgefig('max_depth.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Number and Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as pt\n",
    "\n",
    "#Load Data and Split X/Y; encode target Variable\n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:, 94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifier()\n",
    "n_estimators = [50,100,150,200]\n",
    "max_depth = [2,4,6,8]\n",
    "param_grid = dict(max_depth = max_depth, n_estimators = n_estimators)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_log_loss', n_jobs = -1, cv=kfold, verbose = 1)\n",
    "grid_result = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f (%f) with %r\" %(grid_result.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
