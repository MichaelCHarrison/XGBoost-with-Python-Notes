{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Learning Rate and Number of Trees\n",
    "A problem with gradient boosted decision trees is that they are quick to learn and overfit training data. One effective way to slow down learning in the gradient boosting model is to use a learning rate, also called shrinkage (or eta in XGBoost documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow Learning in Gradient Boosting \n",
    "\n",
    "Gradient boosting involves creating and adding trees to the model sequentially. New trees are created to correct the residual errors in the predictions from the existing sequence of trees. The e↵ect is that the model can quickly fit, then overfit the training dataset. A technique to slow down the learning in the gradient boosting model is to apply a weighting factor for the corrections by new trees when added to the model. This weighting is called the shrinkage factor or the learning rate, depending on the literature or the tool.\n",
    "\n",
    "Naive gradient boosting is the same as gradient boosting with shrinkage where the shrinkage factor is set to 1.0. Setting values less than 1.0 has the e↵ect of making less corrections for each tree added to the model. This in turn results in more trees that must be added to the model. It is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Learning Rate\n",
    "\n",
    "When creating gradient boosting models with XGBoost using the scikit-learn wrapper, the\n",
    "learning rate parameter can be set to control the weighting of new trees added to the model. We can use the grid search capability in scikit-learn to evaluate the e↵ect on logarithmic loss of training a gradient boosting model with di↵erent learning rate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as pt\n",
    "\n",
    "#Load Data and split X/Y; encode Target \n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:,94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifer(nthread=-1)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state=7)\n",
    "learning_rate= [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "grid_params = dict(learning_rate = learning_rate)\n",
    "grid_search = GridSearchCV(model, grid_params, scoring = 'neg_log_loss', cv=kfold)\n",
    "grid_results = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Sumarrize Results\n",
    "print(\"Best %f using %s\" %(grid_results.best_score_, grid_results.best_params))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) using %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "pt.errorbar(learning_rate, means, yerr=stds)\n",
    "pt.title(\"XGBoost learning_rate vs Log Loss\")\n",
    "pt.xlabel(\"learning_rate\")\n",
    "pt.ylabel(\"Log Loss\")\n",
    "pt.savefig(\"learning_rate.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Learning Rate and Number of Trees\n",
    "Smaller learning rates generally require more trees to be added to the model. We can explore this relationship by evaluating a grid of parameter pairs. The number of decision trees will be varied from 100 to 500 and the learning rate varied on a log10 scale from 0.0001 to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgbost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9483248e57a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgbost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgbost'"
     ]
    }
   ],
   "source": [
    "#Load Libraries\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as pt\n",
    "import numpy\n",
    "\n",
    "#Load Data\n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:, 0:94]\n",
    "Y = data.values[:,94]\n",
    "encoded_y = LabelEncoder().fit_transfor(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifier(nthread=-1)\n",
    "kfold = StratifiedKFold(n_split = 10, shuffle = True, random_state = 7)\n",
    "n_estimators = [50,150,200,250]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "param_grid = dict(n_estimators = n_estimators, learning_rate = learning_rate)\n",
    "grid_search = GridSearch(model, param_grid = param_grid, scoring = 'neg_log_loss', n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) using %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot Results\n",
    "scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators))\n",
    "for i, value in enumerate(learning_curve):\n",
    "    pt.plot(n_estimators, scores[i], label = 'learning' + str(value))\n",
    "pt.legend()\n",
    "pt.xlabel('n_estimators')\n",
    "pt.ylabel('learning_rate')\n",
    "py.savefig('n_estimators_vs_learning_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
