{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration advice from R\n",
    "\n",
    "The gradient boosting algorithm is implemented in R as the gbm package. Reviewing the package documentation2, the gbm() function specifies sensible defaults:\n",
    "\n",
    "- n.trees = 100 (number of trees).\n",
    "- interaction.depth = 1 (number of leaves).\n",
    "- n.minobsinnode = 10 (minimum number of samples in tree terminal nodes). \n",
    "- shrinkage = 0.001 (learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration advice from SKLearn\n",
    "\n",
    "The Python library provides an implementation of gradient boosting for classification called the GradientBoostingClassifier class and regression called the GradientBoostingRegressor class. It is useful to review the default configuration for the algorithm in this library. There are many parameters, but below are a few key defaults:\n",
    "\n",
    "- Learning rate = 0.1 (shrinkage).\n",
    "- n estimators = 100 (number of trees). \n",
    "- max depth = 3.\n",
    "- min samples split = 2.\n",
    "- min samples leaf = 1.\n",
    "- subsample = 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration advice from XGBoost\n",
    "\n",
    "The XGBoost library is dedicated to the gradient boosting algorithm. It too specifies default parameters that are interesting to note, firstly the XGBoost Parameters page:\n",
    "\n",
    "- eta = 0.3 (a.k.a learning rate).\n",
    "- max depth = 6.\n",
    "- subsample = 1.\n",
    "- max depth = 3.\n",
    "- learning rate = 0.1.\n",
    "- n estimators = 100.\n",
    "- subsample = 1.\n",
    "\n",
    "## Owen Zhang's configuration tips:\n",
    "\n",
    "- Target 500-to-1000 trees and then tune the learning rate (n estimators).\n",
    "- Set the number of samples in the leaf nodes to enough observations needed to make a good mean estimate (min child weight).\n",
    "- Configure the interaction depth to about 10 or more (max depth).\n",
    "- Number of Trees (n estimators) set to a fixed value between 100 and 1000, depending on the dataset size.\n",
    "- Learning Rate (learningrate) simplified to the ratio: [2 to 10], depending on the trees number of trees.\n",
    "- Row Sampling (subsample) grid searched values in the range [0.5, 0.75, 1.0].\n",
    "- Column Sampling (colsample bytree and maybe colsample bylevel) grid searched values in the range [0.4, 0.6, 0.8, 1.0].\n",
    "- Min Leaf Weight (min child weight) simplified to the ratio 3 , where rare events is the percentage of rare event observations in the dataset.\n",
    "- Tree Size (max depth) grid searched values in the rage [4, 6, 8, 10].\n",
    "- Min Split Gain (gamma) fixed with a value of zero.\n",
    "\n",
    "### Differences that may be relevant\n",
    "\n",
    "- Number of Trees and Learning Rate: Fix the number of trees at around 100 (rather than 1000) and then tune the learning rate.\n",
    "- Max Tree Depth: Start with a value of 6 and presumably tune from there.\n",
    "- Min Leaf Weight: Use a modified ratio of 1 , where rare events is the sqrt(rare events) percentage of rare event observations in the dataset.\n",
    "- Column Sampling: Grid search values in the range of 0.3 to 0.5 (more constrained). Row Sampling: Fixed at the value 1.0.\n",
    "- Min Split Gain: Fixed at the value 0.0.\n",
    "\n",
    "## Abhishek Thakor's tips:\n",
    "The tuning ranges for each parameter are much the same with some notable diâ†µerences. Specifically, he suggests grid searching values for the Min Split Gain (gamma) and the regular- ization penalty terms (reg alpha and reg lambda). He also explore large values for tree size (max depth) values above 10 as well as fixed Min Leaf Weight (min child weight) values in the range of about 1 to 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
