{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Stochastic Gradient Boosting\n",
    "A simple technique for ensembling decision trees involves training trees on subsamples of the training dataset. Subsets of the rows in the training data can be taken to train individual trees called bagging. When subsets of rows of the training data are also taken when calculating each split point, this is called random forest. These techniques can also be used in the gradient tree boosting model in a technique called stochastic gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "Gradient boosting is a greedy procedure. New decision trees are added to the model to correct the residual error of the existing model. Each decision tree is created using a greedy search procedure to select split points that best minimize an objective function. This can result in trees that use the same attributes and even the same split points again and again. Bagging is a technique where a collection of decision trees are created, each from a different random subset of rows from the training data. The effect is that better performance is achieved from the ensemble of trees because the randomness in the sample allows slightly different trees to be created, adding variance to the ensembled predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Row Subsampling\n",
    "\n",
    "Row subsampling involves selecting a random sample of the training dataset without replacement. Row subsampling can be specified in the scikit-learn wrapper of the XGBoost class in the subsample parameter. The default is 1.0 which is no sub-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as pt\n",
    "\n",
    "#Load Data, split, encode target\n",
    "data = read_csv(\"train.csv\")\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:,94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifier(nthread=-1)\n",
    "subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "param_grid = dict(subsample=subsample)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring = 'neg_log_loss', n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f (%f) using %s\" % (grid_result.best_score_, grid_result.best_params))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) using %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot results\n",
    "pt.errorbar(subsample, means, yerr=stds)\n",
    "pt.title(\"XGBoost subsample vs Log Loss\")\n",
    "pt.xlabel(\"Subsample\")\n",
    "pt.ylabel(\"Log Loss\")\n",
    "pt.savefig('subsample.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Column Subsampling by Tree\n",
    "\n",
    "We can also create a random sample of the features (or columns) to use prior to creating each decision tree in the boosted model. In the XGBoost wrapper for scikit-learn, this is controlled by the colsample bytree parameter. The default value is 1.0 meaning that all columns are used in each decision tree. We can evaluate values for colsample bytree between 0.1 and 1.0 incrementing by 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Library\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot at pt\n",
    "\n",
    "#Load data, split, encode\n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:,94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifier(nthread=-1)\n",
    "colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1]\n",
    "param_grid = dict(colsample_bytree = colsample_bytree)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_log_loss', n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f (%f) using %s\" % (grid_result.best_score_, grid_result.best_params))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) using %r\" %(mean, stdev, param))\n",
    "    \n",
    "#Plot\n",
    "pt.errorbar(\"colsample_bytree, means, yerr=stds\")\n",
    "pt.title(\"XGBoost colsample_bytree vs Log Loss\")\n",
    "pt.xlabel('colsample_bytree')\n",
    "pt.ylabel('Log Loss')\n",
    "pt.savefig('colsample_bytree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Column Subsampling By Split\n",
    "Rather than subsample the columns once for each tree, we can subsample them at each split in the decision tree. In principle, this is the approach used in random forest. We can set the size of the sample of columns used at each split in the colsample bylevel parameter in the XGBoost wrapper classes for scikit-learn. As before, we will vary the ratio from 10% to the default of 100%. The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Library\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot at pt\n",
    "\n",
    "#Load data, split, encode\n",
    "data = read_csv('train.csv')\n",
    "X = data.values[:,0:94]\n",
    "Y = data.values[:,94]\n",
    "encoded_y = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "#Grid Search\n",
    "model = XGBClassifier(nthread=-1)\n",
    "colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1]\n",
    "param_grid = dict(colsample_bylevel = colsample_bylevel)\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='neg_log_loss', n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, encoded_y)\n",
    "\n",
    "#Summarize Results\n",
    "print(\"Best: %f (%f) using %s\" % (grid_result.best_score_, grid_result.best_params))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) using %r\" %(mean, stdev, param))\n",
    "    \n",
    "#Plot\n",
    "pt.errorbar(\"colsample_bytree, means, yerr=stds\")\n",
    "pt.title(\"XGBoost colsample_bytree vs Log Loss\")\n",
    "pt.xlabel('colsample_bytree')\n",
    "pt.ylabel('Log Loss')\n",
    "pt.savefig('colsample_bylevel.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
